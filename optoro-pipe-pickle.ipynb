{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebfc5822-2a79-4e4f-aa3c-b7756b0b9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Output,\n",
    "    OutputPath,\n",
    "    Model,\n",
    "    Input,\n",
    "    InputPath,\n",
    "    )\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google.cloud import aiplatform as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b67023a7-57a0-402d-a2cd-f1f82e6ea52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_URI='gs://hardy-rhythm-332916'\n",
    "PROJECT_ID='hardy-rhythm-332916'\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/bikes_weather\".format(BUCKET_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b0892d4-9089-41b6-a33f-f7acda99592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a8e0a6-c57b-4ea5-9366-8474c7c6e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \n",
    "                         \"google-cloud-aiplatform\", \n",
    "                         \"google-cloud-bigquery-storage\",\n",
    "                         \"google-cloud-bigquery\",\n",
    "                         \"pyarrow\"]\n",
    ")\n",
    "def preprocess(in_bigquery_projectid:str, \n",
    "               in_bigquery_dataset:str, \n",
    "               output_csv_path: OutputPath('CSV_DATASET')):\n",
    "    #1\n",
    "    from google.cloud import bigquery\n",
    "    import google.auth\n",
    "    \n",
    "    creds, project = google.auth.default()\n",
    "    client = bigquery.Client(project=in_bigquery_projectid, credentials=creds)\n",
    "\n",
    "    query =     \"\"\"\n",
    "            SELECT * FROM `hardy-rhythm-332916.telco.churn`\n",
    "    \"\"\"\n",
    "    print(query)\n",
    "    \n",
    "    dataframe = client.query(query).to_dataframe()\n",
    "    print(dataframe.head())\n",
    "    \n",
    "    dataframe.to_csv(output_csv_path)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4cf185f-df79-4cef-8cca-99739335a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "   packages_to_install=[\"pandas\", \"scikit-learn==1.0.0\", \"google-cloud-aiplatform\"]\n",
    ")\n",
    "def train(in_experiment_name:str, \n",
    "          in_experiment_training_set: str,\n",
    "          in_vertexai_region: str, \n",
    "          in_vertexai_projectid: str, \n",
    "          in_csv_path: InputPath('CSV_DATASET'), \n",
    "          model_type: str, \n",
    "          saved_model: Output[Model]\n",
    "         ):\n",
    "    \n",
    "    import pandas as pd  \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import sklearn.metrics as metrics\n",
    "    from google.cloud import aiplatform\n",
    "    from datetime import datetime\n",
    "    import joblib\n",
    "    import pickle\n",
    "    import os\n",
    "    import random\n",
    "    idn = random.randint(0,1000)\n",
    "\n",
    "\n",
    "    df = pd.read_csv(in_csv_path)\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype=='object':    #Since we are encoding object datatype to integer/float\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(df[c].values))\n",
    "            df[c] = lbl.transform(df[c].values)\n",
    "    print(df.head())  #To check if properly encoded\n",
    "    \n",
    "    X = df[['Contract', 'tenure', 'TechSupport', 'OnlineSecurity', 'TotalCharges', 'PaperlessBilling',\n",
    "       'DeviceProtection', 'Dependents', 'OnlineBackup', 'SeniorCitizen', 'MonthlyCharges',\n",
    "       'PaymentMethod', 'Partner', 'PhoneService']] #taking only relevant columns\n",
    "    Y = df['Churn']\n",
    "\n",
    "\n",
    "    # Scaling all the variables to a range of 0 to 1\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    features = X.columns.values\n",
    "    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    scaler.fit(X)\n",
    "    X = pd.DataFrame(scaler.transform(X))\n",
    "    X.columns = features\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=101)\n",
    "    \n",
    "    aiplatform.init(\n",
    "       project=in_vertexai_projectid,\n",
    "       location=in_vertexai_region,\n",
    "       experiment=in_experiment_name\n",
    "    )\n",
    "    \n",
    "    run_id = f\"run-{idn}-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    aiplatform.start_run(run_id)\n",
    "    \n",
    "    #Choose which model to train\n",
    "    if model_type == 'svm':\n",
    "        from sklearn import svm\n",
    "        model = svm.LinearSVC()\n",
    "        \n",
    "    elif model_type == 'random_forrest':\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        model = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0)\n",
    "        \n",
    "    elif model_type == 'decision_tree':\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        model = DecisionTreeClassifier()\n",
    "        \n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    head_model_path=os.path.split(saved_model.path)\n",
    "    saved_model_path=os.path.join(head_model_path[0], 'model.pkl')\n",
    "\n",
    "    with open(saved_model_path, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "     \n",
    "    predicted = model.predict(X_test)\n",
    "    \n",
    "    print(\"accuracy: {}\".format(metrics.accuracy_score(Y_test, predicted)))\n",
    "    print(\"f1 score macro: {}\".format(metrics.f1_score(Y_test, predicted, average='macro')   )  )\n",
    "    print(\"f1 score micro: {}\".format(metrics.f1_score(Y_test, predicted, average='micro') ))\n",
    "    print(\"precision score: {}\".format(metrics.precision_score(Y_test, predicted, average='macro') ))\n",
    "    print(\"recall score: {}\".format(metrics.recall_score(Y_test, predicted, average='macro') ))\n",
    "    print(\"hamming_loss: {}\".format(metrics.hamming_loss(Y_test, predicted)))\n",
    "    print(\"log_loss: {}\".format(metrics.log_loss(Y_test, predicted)))\n",
    "    print(\"zero_one_loss: {}\".format(metrics.zero_one_loss(Y_test, predicted)))\n",
    "    print(\"AUC&ROC: {}\".format(metrics.roc_auc_score(Y_test, predicted)))\n",
    "    print(\"matthews_corrcoef: {}\".format(metrics.matthews_corrcoef(Y_test, predicted) ))\n",
    "    \n",
    "    \n",
    "    training_params = {\n",
    "        'training_set': in_experiment_training_set,\n",
    "        'model_type': model_type,\n",
    "        'dataset_path': in_csv_path,\n",
    "        'model_path': saved_model_path\n",
    "    }\n",
    "    \n",
    "    training_metrics = {\n",
    "        'model_accuracy': metrics.accuracy_score(Y_test, predicted),\n",
    "        'model_precision': metrics.precision_score(Y_test, predicted, average='macro'),\n",
    "        'model_recall': metrics.recall_score(Y_test, predicted, average='macro'),\n",
    "        'model_logloss': metrics.log_loss(Y_test, predicted),\n",
    "        'model_auc_roc': metrics.roc_auc_score(Y_test, predicted)\n",
    "    }\n",
    "    \n",
    "    aiplatform.log_params(training_params)\n",
    "    aiplatform.log_metrics(training_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8195b0b3-1cf4-4655-a057-9b2814a0e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "@component(\n",
    "   packages_to_install=[\"pandas\", \"google-cloud-aiplatform\"]\n",
    ")\n",
    "def gate(in_experiment_name: str,\n",
    "         in_experiment_training_set: str,\n",
    "         in_vertexai_region: str,\n",
    "         in_vertexai_projectid: str,\n",
    "         model1: Input[Model],\n",
    "         model2: Input[Model],\n",
    "         model3: Input[Model]\n",
    "        )-> NamedTuple(\n",
    "           'winner_output',\n",
    "            [\n",
    "                ('experiment_info', str),\n",
    "                ('is_current_champion', bool)\n",
    "            ]\n",
    "        ):\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    aiplatform.init(\n",
    "       project=      in_vertexai_projectid,\n",
    "       location=     in_vertexai_region,\n",
    "       experiment =  in_experiment_name\n",
    "    )\n",
    "    \n",
    "    ## get vertex AI model object corresponding to <champion model> from ModelRegistry - use labels: experiment_name \n",
    "    champion_model = None\n",
    "    champion_model_exists = False\n",
    "    \n",
    "    model_filter_str='labels.experiment_name=\"'+in_experiment_name+'\"'\n",
    "    print(\"Model filter string: \"+model_filter_str)\n",
    "    \n",
    "    models = aiplatform.Model.list(\n",
    "        filter=model_filter_str\n",
    "    )\n",
    "    \n",
    "    if len(models)>0:\n",
    "        champion_model_exists = True\n",
    "        champion_model = models[0]\n",
    "        print(champion_model.display_name)\n",
    "        champion_model_experiment_run_id = champion_model.labels['experiment_run_id']\n",
    "    \n",
    "    \n",
    "    ## fetch experiment run details for current <training set>:\n",
    "    experiment_df = aiplatform.get_experiment_df()\n",
    "    experiment_df = experiment_df[experiment_df.experiment_name == in_experiment_name]\n",
    "    \n",
    "    \n",
    "    challengers_experiment_run_info =  experiment_df[experiment_df[\"param.training_set\"] == in_experiment_training_set]\n",
    "    \n",
    "    print(\"Challengers:\")\n",
    "    print(challengers_experiment_run_info.to_string())\n",
    "    \n",
    "    if champion_model != None:\n",
    "       current_champion_experiment_run_info = experiment_df[experiment_df[\"run_name\"] == champion_model_experiment_run_id]\n",
    "    \n",
    "    decision_metric_name = \"metric.model_auc_roc\"\n",
    "    \n",
    "    ### fetch best experiment_run_id from challengers\n",
    "    best_challenger_experiment_run_info = challengers_experiment_run_info[\n",
    "        challengers_experiment_run_info[decision_metric_name]==challengers_experiment_run_info[decision_metric_name].max()\n",
    "    ]\n",
    "    \n",
    "    print(\"Best challenger\")\n",
    "    print(best_challenger_experiment_run_info.to_string())\n",
    "    \n",
    "    winner_experiment_run_info = None\n",
    "    \n",
    "    winner_is_current_champion = False\n",
    "    if champion_model != None: \n",
    "        winner_experiment_run_info = current_champion_experiment_run_info\n",
    "        winner_is_current_champion = True\n",
    "        \n",
    "        ## Final: best_challenger vs champion\n",
    "        if best_challenger_experiment_run_info[decision_metric_name].values[0]>current_champion_experiment_run_info[decision_metric_name].values[0]:\n",
    "            ## best challenger is the new winner\n",
    "            winner_experiment_run_info = best_challenger_experiment_run_info\n",
    "            winner_is_current_champion = False\n",
    "    else: \n",
    "        winner_experiment_run_info = best_challenger_experiment_run_info\n",
    "        winner_is_current_champion = False\n",
    "    \n",
    "    winner_experiment_info = {\n",
    "           \"experiment_name\": winner_experiment_run_info[\"experiment_name\"].values[0],\n",
    "           \"experiment_run_id\": winner_experiment_run_info[\"run_name\"].values[0]\n",
    "    }\n",
    "    \n",
    "    print(\"winner:\")\n",
    "    print(winner_experiment_info)\n",
    "    \n",
    "    ##https://www.kubeflow.org/docs/components/pipelines/sdk/python-function-components/#pass-by-file\n",
    "    winner_namedtuple = namedtuple('winner_output', ['experiment_info', 'is_current_champion'])\n",
    "    \n",
    "    return winner_namedtuple(json.dumps(winner_experiment_info), winner_is_current_champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28dd2a1-0c7e-44c0-a256-171a7a8f71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\",\n",
    "                                \"google-cloud-pipeline-components\",\n",
    "                                \"typing\",\n",
    "                                'datetime'\n",
    "                               ]\n",
    "    \n",
    ")\n",
    "def model(in_experiment_name: str, \n",
    "           in_experiment_training_set: str, \n",
    "           in_vertexai_region: str, \n",
    "           in_vertexai_projectid: str,\n",
    "           eval_info: str, #evaluation_gate_task.outputs['experiment_info']\n",
    "           in_vertex_serving_machine_type: str,\n",
    "           in_vertex_serving_min_replicas: int,\n",
    "           in_vertex_serving_max_replica: int\n",
    "          ) -> str:\n",
    "            \n",
    "        from typing import Union\n",
    "        from typing import Dict\n",
    "        from google.cloud import aiplatform\n",
    "        from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "        import json\n",
    "        \n",
    "        from datetime import datetime\n",
    "        TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "        #x=json.loads(eval_info)\n",
    "        \n",
    "        experiment=json.loads(eval_info)['experiment_name']\n",
    "        run_name=json.loads(eval_info)['experiment_run_id']\n",
    "        \n",
    "        def get_experiment_run_params_sample(\n",
    "            run_name: str,\n",
    "            experiment: Union[str, aiplatform.Experiment],\n",
    "            project: str,\n",
    "            location: str,\n",
    "        ) -> Dict[str, Union[float, int, str]]:\n",
    "            experiment_run = aiplatform.ExperimentRun(\n",
    "                run_name=run_name, experiment=experiment, project=project, location=location\n",
    "            )\n",
    "            return experiment_run.get_params()\n",
    "\n",
    "        results_dict=get_experiment_run_params_sample(run_name, \n",
    "                                 experiment,\n",
    "                                 'hardy-rhythm-332916', \n",
    "                                 'us-central1')\n",
    "            \n",
    "        artifact_uri=results_dict['model_path'].replace('/gcs', 'gs:/').replace('model.pkl','')\n",
    "            \n",
    "        return artifact_uri\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b0777b0-a866-460e-9834-51ab06081a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import pipeline\n",
    "from kfp.dsl import Condition\n",
    "\n",
    "@pipeline(name=\"wf-churn\")\n",
    "def pipeline(\n",
    "    in_bigquery_projectid: str = 'hardy-rhythm-332916',\n",
    "    in_bigquery_dataset: str = 'telcosandbox',\n",
    "    in_corr_threshold: float = 0.05,\n",
    "    in_experiment_name: str = \"telcochurn4\",\n",
    "    in_experiment_training_set: str = \"telcochurn\",\n",
    "    in_vertexai_projectid: str = \"hardy-rhythm-332916\",\n",
    "    in_vertexai_region: str = \"us-central1\",\n",
    "    in_vertex_serving_machine_type: str = \"n1-standard-4\",\n",
    "    in_vertex_serving_min_replicas: int = 1,\n",
    "    in_vertex_serving_max_replicas: int = 2\n",
    "    \n",
    "):\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    #### STEP1: PREPROCESSING\n",
    "    staging_task = preprocess(in_bigquery_projectid,\n",
    "                         in_bigquery_dataset\n",
    "                        )\n",
    "    \n",
    "    \n",
    "    ### STEP2: TRAIN CHALLENGERS\n",
    "    train_task_svm =            train(in_experiment_name, \n",
    "                                      in_experiment_training_set, \n",
    "                                      in_vertexai_region, \n",
    "                                      in_vertexai_projectid, \n",
    "                                      staging_task.output, \n",
    "                                      'svm')\n",
    "    train_task_random_forrest = train(in_experiment_name,\n",
    "                                      in_experiment_training_set,\n",
    "                                      in_vertexai_region, \n",
    "                                      in_vertexai_projectid, \n",
    "                                      staging_task.output, \n",
    "                                      'random_forrest')\n",
    "    train_task_decision_tree = train(in_experiment_name, \n",
    "                                      in_experiment_training_set, \n",
    "                                      in_vertexai_region, \n",
    "                                      in_vertexai_projectid, \n",
    "                                      staging_task.output, \n",
    "                                      'decision_tree')\n",
    "    \n",
    "    \n",
    "    #### STEP3: GATE - Identify best challenger and compare with current champion\n",
    "    evaluation_gate_task = gate(in_experiment_name, \n",
    "                                in_experiment_training_set, \n",
    "                                in_vertexai_region, \n",
    "                                in_vertexai_projectid, \n",
    "                                train_task_svm.output, \n",
    "                                train_task_random_forrest.output, \n",
    "                                train_task_decision_tree.output)\n",
    "     \n",
    "    \n",
    "    with Condition(\n",
    "        evaluation_gate_task.outputs['is_current_champion'] == \"false\", \n",
    "        name=\"deploy_new_champion\"\n",
    "    ): \n",
    "        ### STEP 5&6 Register new Chamption and deploy it to endpoint\n",
    "        model_task = model(in_experiment_name, \n",
    "                        in_experiment_training_set, \n",
    "                        in_vertexai_region, \n",
    "                        in_vertexai_projectid,\n",
    "                        evaluation_gate_task.outputs['experiment_info'],\n",
    "                        in_vertex_serving_machine_type,\n",
    "                        in_vertex_serving_min_replicas,\n",
    "                        in_vertex_serving_max_replicas\n",
    "                       )\n",
    "        \n",
    "        model_upload_op = gcc_aip.ModelUploadOp(\n",
    "            project=in_vertexai_projectid,\n",
    "            display_name=\"model\"+TIMESTAMP, \n",
    "            artifact_uri=model_task.output, # GCS location of model\n",
    "            serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\"\n",
    "        )\n",
    "\n",
    "        endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "            project=in_vertexai_projectid,\n",
    "            display_name=\"pipelines\"+TIMESTAMP,\n",
    "        )\n",
    "\n",
    "        model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "            endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "            model=model_upload_op.outputs[\"model\"],\n",
    "            deployed_model_display_name=\"model_display_name\",\n",
    "            dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "            dedicated_resources_min_replica_count=in_vertex_serving_min_replicas\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db2ee4e5-6aca-4e4e-871d-e9adf6e8718e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"custom_model_training_spec.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "726ed112-8252-4af9-9402-669393a86601",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"cifar10_\" + TIMESTAMP\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"custom_model_training_spec.json\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af7e3a2d-878f-4bde-848d-0178f0be4f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/207851070780/locations/us-central1/pipelineJobs/wf-churn-20220929174943\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/207851070780/locations/us-central1/pipelineJobs/wf-churn-20220929174943')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/wf-churn-20220929174943?project=207851070780\n",
      "PipelineJob projects/207851070780/locations/us-central1/pipelineJobs/wf-churn-20220929174943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/207851070780/locations/us-central1/pipelineJobs/wf-churn-20220929174943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/207851070780/locations/us-central1/pipelineJobs/wf-churn-20220929174943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/207851070780/locations/us-central1/pipelineJobs/wf-churn-20220929174943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/207851070780/locations/us-central1/pipelineJobs/wf-churn-20220929174943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/207851070780/locations/us-central1/pipelineJobs/wf-churn-20220929174943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/207851070780/locations/us-central1/pipelineJobs/wf-churn-20220929174943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/207851070780/locations/us-central1/pipelineJobs/wf-churn-20220929174943\n"
     ]
    }
   ],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539c77c-019f-497a-8d2b-0e4e6f62889f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mp_tfx.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "environment": {
      "name": "tf2-2-3-gpu.2-3.m56",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPxSVqcvqPHO"
      },
      "source": [
        "# TFX SDK examples\n",
        "\n",
        "This notebook contains a series of related examples based on the \"Chicago Taxi Pipeline\", that show using the TFX SDK. It includes examples of how to use custom Python functions and custom containers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-Av6cm0oBFV"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Before you run this notebook, ensure that your Google Cloud user account and project are granted access to the Managed Pipelines Experimental. To be granted access to the Managed Pipelines Experimental, fill out this [form](http://go/cloud-mlpipelines-signup) and let your account representative know you have requested access. \n",
        "\n",
        "This notebook is intended to be run on [AI Platform Notebooks](https://cloud.google.com/ai-platform-notebooks). See the \"AI Platform Notebooks\" section in the Experimental [User Guide](https://docs.google.com/document/d/1JXtowHwppgyghnj1N1CT73hwD1caKtWkLcm2_0qGBoI/edit?usp=sharing) for more detail on creating a notebook server instance.\n",
        "\n",
        "**To run this notebook on AI Platform Notebooks**, click on the **File** menu, then select \"Download .ipynb\".  Then, upload that notebook from your local machine to AI Platform Notebooks. (In the AI Platform Notebooks left panel, look for an icon of an arrow pointing up, to upload).\n",
        "\n",
        "The notebook will probably run on [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb) as well, though currently you may see some ignorable warnings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwZ0aXisoBFW"
      },
      "source": [
        "We'll first install some libraries and set some variables.\n",
        "\n",
        "Ensure python 3 is being used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ-QwavmqPHP",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8b8e258c-7f2e-4184-f7ae-6f06cb021fdf"
      },
      "source": [
        "import sys\n",
        "sys.version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.6.9 (default, Oct  8 2020, 12:12:24) \\n[GCC 8.4.0]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ-GWdI7SmrN"
      },
      "source": [
        "Set `gcloud` to use your project.  **Edit the following cell before running it**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD5jOcSURdcU"
      },
      "source": [
        "PROJECT_ID = 'your-project-id'  # <---CHANGE THIS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAaCPLjgiJrO"
      },
      "source": [
        "Set `gcloud` to use your project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkWdxe4TXRHk"
      },
      "source": [
        "!gcloud config set project {PROJECT_ID}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gckGHdW9iPrq"
      },
      "source": [
        "If you're running this notebook on colab, authenticate with your user account:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZQA0KrfXCvU"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaqJjbmk6o0o"
      },
      "source": [
        "-----------------\n",
        "\n",
        "**If you're on AI Platform Notebooks**, authenticate with Google Cloud before running the next section, by running\n",
        "```sh\n",
        "gcloud auth login\n",
        "```\n",
        "**in the Terminal window** (which you can open via **File** > **New** in the menu). You only need to do this once per notebook instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F08xSY0qPHV"
      },
      "source": [
        "!gsutil cp gs://cloud-aiplatform-pipelines/releases/20201123/aiplatform_pipelines_client-0.1.0.caip20201123-py3-none-any.whl ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQJl_6EUoBFr"
      },
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "  USER_FLAG = ''\n",
        "else:\n",
        "  USER_FLAG = '--user'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyQtljP-qPHY"
      },
      "source": [
        "!pip install {USER_FLAG} pip==20.2.4 --upgrade\n",
        "!pip install {USER_FLAG} kfp tfx==0.26.0 aiplatform_pipelines_client-0.1.0.caip20201123-py3-none-any.whl --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZeSQybKwbpp"
      },
      "source": [
        "If you're on colab, and you got a prompt to restart the runtime after TFX installation, rerun some setup now before proceeding. As before, **edit the following to define your project id** before you run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpkxFp93xBk5"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  PROJECT_ID = 'your-project-id'  # <---CHANGE THIS\n",
        "  !gcloud config set project {PROJECT_ID}\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  USER_FLAG = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHTSzMygoBF6"
      },
      "source": [
        "if not 'google.colab' in sys.modules:\n",
        "  # Automatically restart kernel after installs\n",
        "  import IPython\n",
        "  app = IPython.Application.instance()\n",
        "  app.kernel.do_shutdown(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDnPgN8UJtzN"
      },
      "source": [
        "Check the TFX version. It should be == 0.26.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jh7vKSRqPHb"
      },
      "source": [
        "# Check version\n",
        "!pip show tfx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDtLdSkvqPHe"
      },
      "source": [
        "### Setup variables\n",
        "\n",
        "Let's set up some variables used to customize the pipelines below.  **Edit the values in the cell below before running it**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcUseqJaE2XN"
      },
      "source": [
        "PATH=%env PATH\n",
        "%env PATH={PATH}:/home/jupyter/.local/bin\n",
        "\n",
        "# Required Parameters\n",
        "USER = 'YOUR_LDAP' # <---CHANGE THIS\n",
        "BUCKET_NAME = 'YOUR_BUCKET_NAME'  # <---CHANGE THIS\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER)\n",
        "\n",
        "PROJECT_ID = 'YOUR_PROJECT_ID'  # <---CHANGE THIS\n",
        "REGION = 'us-central1'\n",
        "API_KEY = 'YOUR_API_KEY'  # <---CHANGE THIS\n",
        "\n",
        "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvb0SspyqPH4"
      },
      "source": [
        "## Part 2: Custom Python functions\n",
        "\n",
        "In this section, we will create components from Python functions. We won't be doing any real MLâ€” these simple functions are just used to illustrate the process.\n",
        "\n",
        "We'll use [Cloud Build](https://cloud.google.com/cloud-build) to build the container image that runs the functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08r9af-yoBGv"
      },
      "source": [
        "!mkdir -p custom_fns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaEJd_sYoBGy"
      },
      "source": [
        "%cd custom_fns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhYjn9Fj6mdo"
      },
      "source": [
        "We begin by writing a preprocessing function that enables the user to specify different split fractions between training and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHNtKTuiqPH4"
      },
      "source": [
        "%%writefile my_preprocess.py\n",
        "\n",
        "import os\n",
        "import tensorflow as tf  # Used for writing files.\n",
        "\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import OutputArtifact\n",
        "\n",
        "@component\n",
        "def MyPreprocess(training_data: OutputArtifact[Dataset]):\n",
        "  with tf.io.gfile.GFile(os.path.join(training_data.uri, 'training_data_file.txt'), 'w') as f:\n",
        "    f.write('Dummy training data')\n",
        "    \n",
        "  # We'll modify metadata and ensure that it gets passed to downstream components.  \n",
        "  training_data.set_string_custom_property('my_custom_field', 'my_custom_value')\n",
        "  training_data.set_string_custom_property('uri_for_output', training_data.uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtapXcbSqPH6"
      },
      "source": [
        "Let's write a second component that uses the training data produced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ZEf2xQqPH7"
      },
      "source": [
        "%%writefile my_trainer.py\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "from tfx.types.standard_artifacts import Model\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import InputArtifact, OutputArtifact, Parameter\n",
        "\n",
        "\n",
        "@component\n",
        "def MyTraining(training_data: InputArtifact[Dataset],\n",
        "               model: OutputArtifact[Model],\n",
        "               num_iterations: Parameter[int] = 100):\n",
        "  # Let's read the contents of training data and write to the metadata.\n",
        "  with tf.io.gfile.GFile(\n",
        "      os.path.join(training_data.uri, 'training_data_file.txt'), 'r') as f:\n",
        "    contents = f.read()\n",
        "    model.set_string_custom_property('contents_of_training_data', contents)\n",
        "    model.set_int_custom_property('num_iterations_used', num_iterations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJJma2rNqPH9"
      },
      "source": [
        "Let's write a finalizer component that collects all metadata, and dumps it. Ensure that PIPELINE_ROOT is correctly defined before creating the template."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw3ZfNyu7GSg"
      },
      "source": [
        "PIPELINE_ROOT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XRaIluaqPH9"
      },
      "source": [
        "collector_template = f\"\"\"\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "from google.protobuf import json_format\n",
        "\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "from tfx.types.standard_artifacts import Model\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import *\n",
        "from tfx.utils import json_utils\n",
        "\n",
        "OUTPUT_LOCATION = '{PIPELINE_ROOT}/python_function_pipeline/metadata.json'\n",
        "\n",
        "@component\n",
        "def MetadataCollector(training_data: InputArtifact[Dataset],\n",
        "                      model: InputArtifact[Model]):\n",
        "  artifacts = [\n",
        "      json_format.MessageToDict(x)\n",
        "      for x in [training_data.mlmd_artifact, model.mlmd_artifact]\n",
        "  ]\n",
        "  with tf.io.gfile.GFile(OUTPUT_LOCATION, 'w') as f:\n",
        "    f.write(json.dumps(artifacts, indent=4))\n",
        "\"\"\"\n",
        "with open('metadata_collector.py', 'w') as f:\n",
        "    f.write(collector_template)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0Nno65wqPH_"
      },
      "source": [
        "Next, let's package the above into a container. We'll do this manually using a Dockerfile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB4yCHshqPIE"
      },
      "source": [
        "%%writefile Dockerfile\n",
        "FROM gcr.io/tfx-oss-public/tfx:0.26.0\n",
        "WORKDIR /pipeline\n",
        "COPY ./ ./\n",
        "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BippqtrLoBHI"
      },
      "source": [
        "Next, we'll use Cloud Build to build the container image and upload it to GCR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPmVRi7L8JOS"
      },
      "source": [
        "!echo $PROJECT_ID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxiLqXxPqPH_"
      },
      "source": [
        "!gcloud builds submit --tag gcr.io/{PROJECT_ID}/caip-tfx-custom:{USER} ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIrGHQzFqPII"
      },
      "source": [
        "Next, let's author a pipeline using these components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j43snQpRqPII"
      },
      "source": [
        "import os\n",
        "\n",
        "# Only required for local run.\n",
        "from tfx.orchestration.metadata import sqlite_metadata_connection_config\n",
        "\n",
        "from tfx.orchestration.pipeline import Pipeline\n",
        "from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner\n",
        "\n",
        "from my_preprocess import MyPreprocess\n",
        "from my_trainer import MyTraining\n",
        "from metadata_collector import MetadataCollector\n",
        "\n",
        "PIPELINE_NAME = \"function-based-pipeline-{}\".format(USER)\n",
        "\n",
        "def function_based_pipeline(pipeline_root):\n",
        "    preprocess = MyPreprocess()\n",
        "\n",
        "    training = MyTraining(\n",
        "        training_data=preprocess.outputs['training_data'],\n",
        "        num_iterations=10000)\n",
        "    \n",
        "    collect = MetadataCollector(\n",
        "        training_data=preprocess.outputs['training_data'],\n",
        "        model=training.outputs['model'])\n",
        "    \n",
        "    pipeline_name = \"function-based-pipeline-{}\".format(USER)\n",
        "    return Pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=pipeline_root,\n",
        "        # Only needed for local runs.\n",
        "        metadata_connection_config=sqlite_metadata_connection_config('metadata.sqlite'),\n",
        "        components=[preprocess, training, collect])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzyAQJrUqPIK"
      },
      "source": [
        "Let's make sure this pipeline works locally, using the local runner:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgBSXC12qPIL"
      },
      "source": [
        "from tfx.orchestration.local.local_dag_runner import LocalDagRunner\n",
        "\n",
        "LocalDagRunner().run(function_based_pipeline(pipeline_root='/tmp/pipeline_root'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofs15ZiWqPIN"
      },
      "source": [
        "Check that the metadata was produced locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXUzLgYUqPIN"
      },
      "source": [
        "from ml_metadata import metadata_store\n",
        "from ml_metadata.proto import metadata_store_pb2\n",
        "\n",
        "connection_config = metadata_store_pb2.ConnectionConfig()\n",
        "connection_config.sqlite.filename_uri = 'metadata.sqlite'\n",
        "connection_config.sqlite.connection_mode = 3 # READWRITE_OPENCREATE\n",
        "store = metadata_store.MetadataStore(connection_config)\n",
        "store.get_artifacts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWlObcW9qPIP"
      },
      "source": [
        "### Run the pipeline with Managed Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwXwGvrV6Rcc"
      },
      "source": [
        "Now we're ready to run the pipeline!  We're constructing the client using the API_KEY that you created during setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehuX7MUt6Rcf"
      },
      "source": [
        "from aiplatform.pipelines import client\n",
        "\n",
        "my_client = client.Client(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    api_key=API_KEY\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTCk8EkRqPIQ"
      },
      "source": [
        "config = kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(\n",
        "    project_id=PROJECT_ID,\n",
        "    display_name='function-based-pipeline-{}'.format(USER),\n",
        "    default_image='gcr.io/{}/caip-tfx-custom:{}'.format(PROJECT_ID, USER))\n",
        "runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(\n",
        "    config=config,\n",
        "    output_filename='pipeline.json')\n",
        "runner.compile(\n",
        "    function_based_pipeline(\n",
        "        pipeline_root=os.path.join(PIPELINE_ROOT, PIPELINE_NAME)),\n",
        "    write_out=True)\n",
        "my_client.create_run_from_job_spec('pipeline.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHhmznCBqPIT"
      },
      "source": [
        "When the pipeline is complete, we can check the final output file to see the metadata produced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQdAv3RKqPIU"
      },
      "source": [
        "MD_URI = 'gs://{}/pipeline_root/{}/python_function_pipeline/metadata.json'.format(BUCKET_NAME, USER)\n",
        "MD_URI\n",
        "\n",
        "!gsutil cat {MD_URI}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiSY2vLx7SbV"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCaF4s3dqPIV"
      },
      "source": [
        "## Part 3: Custom containers\n",
        "\n",
        "\n",
        "In this section, we will build custom containers and chain them together as a pipeline.\n",
        "\n",
        "This illustrates how we can pass data (using uris) to custom containers. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvuOnyfg5dzY"
      },
      "source": [
        "\n",
        "### Container 1: Generate examples\n",
        "\n",
        "First, we'll define and write out the `generate_examples.py` code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyrXfan--u6o"
      },
      "source": [
        "!mkdir -p generate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4mRhnC9qPIW"
      },
      "source": [
        "%%writefile generate/generate_examples.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "def _serialize_example(example, label):\n",
        "  example_value = tf.io.serialize_tensor(example).numpy()\n",
        "  label_value = tf.io.serialize_tensor(label).numpy()\n",
        "  feature = {\n",
        "      'examples':\n",
        "          tf.train.Feature(\n",
        "              bytes_list=tf.train.BytesList(value=[example_value])),\n",
        "      'labels':\n",
        "          tf.train.Feature(bytes_list=tf.train.BytesList(value=[label_value])),\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(\n",
        "      feature=feature)).SerializeToString()\n",
        "\n",
        "\n",
        "def _tf_serialize_example(example, label):\n",
        "  serialized_tensor = tf.py_function(_serialize_example, (example, label),\n",
        "                                     tf.string)\n",
        "  return tf.reshape(serialized_tensor, ())\n",
        "\n",
        "\n",
        "def generate_examples(training_data_uri, test_data_uri, config_file_uri):\n",
        "  (train_data, test_data), info = tfds.load(\n",
        "      # Use the version pre-encoded with an ~8k vocabulary.\n",
        "      'imdb_reviews/subwords8k',\n",
        "      # Return the train/test datasets as a tuple.\n",
        "      split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "      # Return (example, label) pairs from the dataset (instead of a dictionary).\n",
        "      as_supervised=True,\n",
        "      with_info=True)\n",
        "\n",
        "  serialized_train_examples = train_data.map(_tf_serialize_example)\n",
        "  serialized_test_examples = test_data.map(_tf_serialize_example)\n",
        "\n",
        "  filename = os.path.join(training_data_uri, \"train.tfrecord\")\n",
        "  writer = tf.data.experimental.TFRecordWriter(filename)\n",
        "  writer.write(serialized_train_examples)\n",
        "\n",
        "  filename = os.path.join(test_data_uri, \"test.tfrecord\")\n",
        "  writer = tf.data.experimental.TFRecordWriter(filename)\n",
        "  writer.write(serialized_test_examples)\n",
        "\n",
        "  encoder = info.features['text'].encoder\n",
        "  config = {\n",
        "      'vocab_size': encoder.vocab_size,\n",
        "  }\n",
        "  config_file = os.path.join(config_file_uri, \"config\")\n",
        "  with tf.io.gfile.GFile(config_file, 'w') as f:\n",
        "    f.write(json.dumps(config))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--training_data_uri', type=str)\n",
        "  parser.add_argument('--test_data_uri', type=str)\n",
        "  parser.add_argument('--config_file_uri', type=str)\n",
        "\n",
        "  args = parser.parse_args()\n",
        "  generate_examples(args.training_data_uri, args.test_data_uri,\n",
        "                    args.config_file_uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvseVvSDqPIX"
      },
      "source": [
        "Next, we'll create a Dockerfile that builds a container to run `generate_examples.py`. Here we use a Google DLVM container image as our base. You may use your own image as the base image as well. Note that we're also installing the `tensorflow_datasets` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymsKgvwkqPIY"
      },
      "source": [
        "%%writefile generate/Dockerfile\n",
        "\n",
        "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\n",
        "WORKDIR /pipeline\n",
        "COPY generate_examples.py generate_examples.py\n",
        "RUN pip install tensorflow_datasets\n",
        "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAcPO09H-90d"
      },
      "source": [
        "!gcloud builds submit --tag gcr.io/{PROJECT_ID}/caip-tfx-custom-container-generate:{USER} generate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH541EW0qPId"
      },
      "source": [
        "### Container 2: Train Examples\n",
        "Next, we'll do the same for the 'Train Examples' custom container. We'll first write out a `train_examples.py` file, then build a container that runs it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-LOa4Fr_ILY"
      },
      "source": [
        "!mkdir -p train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFc8KfNBqPId"
      },
      "source": [
        "%%writefile train/train_examples.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def _parse_example(record):\n",
        "  f = {\n",
        "      'examples': tf.io.FixedLenFeature((), tf.string, default_value=''),\n",
        "      'labels': tf.io.FixedLenFeature((), tf.string, default_value='')\n",
        "  }\n",
        "  return tf.io.parse_single_example(record, f)\n",
        "\n",
        "\n",
        "def _to_tensor(record):\n",
        "  examples = tf.io.parse_tensor(record['examples'], tf.int64)\n",
        "  labels = tf.io.parse_tensor(record['labels'], tf.int64)\n",
        "  return (examples, labels)\n",
        "\n",
        "\n",
        "def train_examples(training_data_uri, test_data_uri, config_file_uri,\n",
        "                   output_model_uri, output_metrics_uri):\n",
        "  train_examples = tf.data.TFRecordDataset(\n",
        "      [os.path.join(training_data_uri, 'train.tfrecord')])\n",
        "  test_examples = tf.data.TFRecordDataset(\n",
        "      [os.path.join(test_data_uri, 'test.tfrecord')])\n",
        "\n",
        "  train_batches = train_examples.map(_parse_example).map(_to_tensor)\n",
        "  test_batches = test_examples.map(_parse_example).map(_to_tensor)\n",
        "\n",
        "  with tf.io.gfile.GFile(os.path.join(config_file_uri, 'config')) as f:\n",
        "    config = json.loads(f.read())\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(config['vocab_size'], 16),\n",
        "      tf.keras.layers.GlobalAveragePooling1D(),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  model.compile(\n",
        "      optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  train_batches = train_batches.shuffle(1000).padded_batch(\n",
        "      32, (tf.TensorShape([None]), tf.TensorShape([])))\n",
        "\n",
        "  test_batches = test_batches.padded_batch(\n",
        "      32, (tf.TensorShape([None]), tf.TensorShape([])))\n",
        "\n",
        "  history = model.fit(\n",
        "      train_batches,\n",
        "      epochs=10,\n",
        "      validation_data=test_batches,\n",
        "      validation_steps=30)\n",
        "\n",
        "  loss, accuracy = model.evaluate(test_batches)\n",
        "\n",
        "  metrics = {\n",
        "      'loss': str(loss),\n",
        "      'accuracy': str(accuracy),\n",
        "  }\n",
        "\n",
        "  model_json = model.to_json()\n",
        "  with tf.io.gfile.GFile(os.path.join(output_model_uri, 'model.json'),\n",
        "                         'w') as f:\n",
        "    f.write(model_json)\n",
        "\n",
        "  with tf.io.gfile.GFile(os.path.join(output_metrics_uri, 'metrics.json'),\n",
        "                         'w') as f:\n",
        "    f.write(json.dumps(metrics))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--training_data_uri', type=str)\n",
        "  parser.add_argument('--test_data_uri', type=str)\n",
        "  parser.add_argument('--config_file_uri', type=str)\n",
        "  parser.add_argument('--output_model_uri', type=str)\n",
        "  parser.add_argument('--output_metrics_uri', type=str)\n",
        "\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  train_examples(args.training_data_uri, args.test_data_uri,\n",
        "                 args.config_file_uri, args.output_model_uri,\n",
        "                 args.output_metrics_uri)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYaUI5gaqPIf"
      },
      "source": [
        "Next, we'll create a Dockerfile that builds a container to run train_examples.py:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaGrpO6gqPIf"
      },
      "source": [
        "%%writefile train/Dockerfile\n",
        "\n",
        "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\n",
        "WORKDIR /pipeline\n",
        "COPY train_examples.py train_examples.py\n",
        "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETZl3Hkz_Spr"
      },
      "source": [
        "!gcloud builds submit --tag gcr.io/{PROJECT_ID}/caip-tfx-custom-container-train:{USER} train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljvt7BTtqPIk"
      },
      "source": [
        "### Define a container-based pipeline\n",
        "\n",
        "Now we're ready to define a pipeline that uses these containers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwibfgFNqPIk"
      },
      "source": [
        "import os\n",
        "\n",
        "from tfx.orchestration.pipeline import Pipeline\n",
        "from tfx.types.standard_artifacts import Model\n",
        "from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "from tfx.types.experimental.simple_artifacts import File\n",
        "from tfx.types.experimental.simple_artifacts import Metrics\n",
        "from tfx.dsl.component.experimental.container_component import create_container_component\n",
        "from tfx.dsl.component.experimental.placeholders import InputUriPlaceholder\n",
        "from tfx.dsl.component.experimental.placeholders import OutputUriPlaceholder\n",
        "\n",
        "\n",
        "import absl\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "\n",
        "\n",
        "def container_based_pipeline():\n",
        "  generate = create_container_component(\n",
        "      name='GenerateExamples',\n",
        "      outputs={\n",
        "          'training_data': Dataset,\n",
        "          'test_data': Dataset,\n",
        "          'config_file': File,\n",
        "      },\n",
        "      image = 'gcr.io/{}/caip-tfx-custom-container-generate:{}'.format(PROJECT_ID, USER),\n",
        "      command=[\n",
        "        'python', '/pipeline/generate_examples.py',\n",
        "        '--training_data_uri', OutputUriPlaceholder('training_data'),\n",
        "        '--test_data_uri', OutputUriPlaceholder('test_data'),\n",
        "        '--config_file_uri', OutputUriPlaceholder('config_file'),\n",
        "      ])\n",
        "\n",
        "  train = create_container_component(\n",
        "      name='Train',\n",
        "      inputs={\n",
        "          'training_data': Dataset,\n",
        "          'test_data': Dataset,\n",
        "          'config_file': File,\n",
        "      },\n",
        "      outputs={\n",
        "          'model': Model,\n",
        "          'metrics': Metrics,\n",
        "      },\n",
        "      image='gcr.io/{}/caip-tfx-custom-container-train:{}'.format(PROJECT_ID, USER),\n",
        "      command=[\n",
        "        'python', '/pipeline/train_examples.py',\n",
        "        '--training_data_uri', InputUriPlaceholder('training_data'),\n",
        "        '--test_data_uri', InputUriPlaceholder('test_data'),\n",
        "        '--config_file_uri', InputUriPlaceholder('config_file'),\n",
        "        '--output_model_uri', OutputUriPlaceholder('model'),\n",
        "        '--output_metrics_uri', OutputUriPlaceholder('metrics'),\n",
        "      ])\n",
        "\n",
        "  generate_component = generate()\n",
        "  train_component = train(\n",
        "    training_data=generate_component.outputs['training_data'],\n",
        "    test_data=generate_component.outputs['test_data'],\n",
        "    config_file=generate_component.outputs['config_file'])\n",
        "\n",
        "  pipeline_name = \"container-based-pipeline-{}\".format(USER)\n",
        "  return Pipeline(\n",
        "        pipeline_name=pipeline_name,\n",
        "        enable_cache=True,\n",
        "        pipeline_root=os.path.join(PIPELINE_ROOT, pipeline_name),\n",
        "        components=[generate_component, train_component])\n",
        "\n",
        "container_based_pipeline = container_based_pipeline()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm1_zvCpqPIm"
      },
      "source": [
        "*Now* let's run the pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sDWS15mqPIm"
      },
      "source": [
        "config = kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(\n",
        "    project_id=PROJECT_ID,\n",
        "    display_name='container-based-pipeline-{}'.format(USER))\n",
        "runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(\n",
        "    config=config,\n",
        "    output_filename='pipeline.json')\n",
        "runner.compile(\n",
        "    container_based_pipeline,\n",
        "    write_out=True)\n",
        "my_client.create_run_from_job_spec('pipeline.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjcJibzaqPIq"
      },
      "source": [
        "## Part 4: Caching\n",
        "In Part 3, the pipeline was run with the cache enabled. Let's try to run the same pipeline again after the one above has finished. We should see it complete immediately since all steps were cached (which you can tell from the green arrow displayed for the components that used caching):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtVq5j6fqPIr"
      },
      "source": [
        "my_client.create_run_from_job_spec('pipeline.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV69L_dRaEJr"
      },
      "source": [
        "<a href=\"https://storage.googleapis.com/amy-jo/images/kf-pls/tfx_cached.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kf-pls/tfx_cached.png\" width=\"40%\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBGhTAgZqPIt"
      },
      "source": [
        "Now, let's disable the cache and run it again. This time, it should re-run all steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rDVTDdhqPIt"
      },
      "source": [
        "container_based_pipeline.enable_cache = False\n",
        "runner.compile(container_based_pipeline, write_out=True)\n",
        "my_client.create_run_from_job_spec('pipeline.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1v_yRk-qPIv"
      },
      "source": [
        "## Part 5: Specifying Task-based dependencies\n",
        "\n",
        "In this section, we will run two steps of a pipeline using task-based dependencies (rather than I/O dependencies) to schedule them. We'll build and use the same container for both steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqTIUEHiFV_0"
      },
      "source": [
        "!mkdir -p task_based"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x3i6hleqPIv"
      },
      "source": [
        "%%writefile task_based/task_based_step.py\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tfx.types.experimental.simple_artifacts import File\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import OutputArtifact, Parameter\n",
        "\n",
        "\n",
        "@component\n",
        "def MyTaskBasedStep(\n",
        "    output_file: OutputArtifact[File], \n",
        "    step_number: Parameter[int] = 0, \n",
        "    contents: Parameter[str] = ''):\n",
        "  # Write out whatever string was passed in to the file.\n",
        "  \n",
        "  with tf.io.gfile.GFile(os.path.join(output_file.uri, 'output.txt'), 'w') as f:\n",
        "    f.write('Step {}: Contents: {}'.format(step_number, contents))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ABNuNhqPIx"
      },
      "source": [
        "Write out the docker file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETCaQQu-qPIy"
      },
      "source": [
        "%%writefile task_based/Dockerfile\n",
        "\n",
        "FROM gcr.io/tfx-oss-public/tfx:0.26.0\n",
        "WORKDIR /pipeline\n",
        "COPY task_based_step.py task_based_step.py\n",
        "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xlPjxjOqPI2"
      },
      "source": [
        "Build a container image for the new component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AQpnBPnFeG0"
      },
      "source": [
        "!gcloud builds submit --tag gcr.io/{PROJECT_ID}/caip-tfx-custom-task-based:{USER} task_based"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4P-rwRLqPI4"
      },
      "source": [
        "Let's create a pipeline with this simple component. Note the `add_upstream_node` call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX91D9JJHVgN"
      },
      "source": [
        "%cd task_based"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnJ7aiQWqPI4"
      },
      "source": [
        "import os\n",
        "\n",
        "from tfx.orchestration.pipeline import Pipeline\n",
        "from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner\n",
        "\n",
        "from task_based_step import MyTaskBasedStep\n",
        "\n",
        "def task_based_pipeline():\n",
        "    step_1 = MyTaskBasedStep(step_number=1, contents=\"This is step 1\")\n",
        "    step_2 = MyTaskBasedStep(\n",
        "        step_number=2, \n",
        "        contents=\"This is step 2\", \n",
        "        instance_name='MyTaskBasedStep2')\n",
        "    step_2.add_upstream_node(step_1)\n",
        "     \n",
        "    pipeline_name = \"task-dependency-based-pipeline-{}\".format(USER)\n",
        "    return Pipeline(\n",
        "        pipeline_name=pipeline_name,\n",
        "        pipeline_root=os.path.join(PIPELINE_ROOT, pipeline_name),\n",
        "        components=[step_1, step_2])\n",
        "\n",
        "task_based_pipeline = task_based_pipeline()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pcq62-IbHc3"
      },
      "source": [
        "Run the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrEZzwrPqPI6"
      },
      "source": [
        "config = kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(\n",
        "    project_id=PROJECT_ID,\n",
        "    default_image='gcr.io/{}/caip-tfx-custom-task-based:{}'.format(PROJECT_ID, USER))\n",
        "runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(\n",
        "    config=config,\n",
        "    output_filename='pipeline.json')\n",
        "runner.compile(\n",
        "    task_based_pipeline,\n",
        "    write_out=True)\n",
        "my_client.create_run_from_job_spec('pipeline.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLr2uixy7cc2"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuIN0GPeqPHh"
      },
      "source": [
        "## Part 6: Chicago Taxi Pipeline\n",
        "\n",
        "In this section, we'll run the canonical Chicago Taxi Pipeline.\n",
        "\n",
        "### What's new\n",
        "\n",
        "If you're familiar with the previous version, here's what's new:\n",
        "\n",
        "- Support for resolvers: LatestArtifactResolver and LatestBlessedModelResolver\n",
        "- FileBasedExampleGen\n",
        "- A Python client SDK to talk to the Alpha service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE3r-NU_yp9m"
      },
      "source": [
        "We'll first do some imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5RHVqCvqPHh"
      },
      "source": [
        "from typing import Any, Dict, List, Optional, Text\n",
        "\n",
        "import absl\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "from tfx.extensions.google_cloud_big_query.example_gen.component import BigQueryExampleGen\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import Evaluator\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import InfraValidator\n",
        "from tfx.components import Pusher\n",
        "from tfx.components import ResolverNode\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Trainer\n",
        "from tfx.components import Transform\n",
        "from tfx.dsl.experimental import latest_artifacts_resolver\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
        "from tfx.orchestration import pipeline as tfx_pipeline\n",
        "from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.types import standard_artifacts\n",
        "from tfx.utils import dsl_utils\n",
        "from tfx.types import channel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2VR2BM-yuKY"
      },
      "source": [
        "Next, we'll define the [BigQuery](https://cloud.google.com/bigquery/docs) query that we want to use with the `BigQueryExampleGen` component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbnWZCLwqPHk"
      },
      "source": [
        "# Define the query used for BigQueryExampleGen.\n",
        "QUERY = \"\"\"\n",
        "        SELECT\n",
        "          pickup_community_area,\n",
        "          fare,\n",
        "          EXTRACT(MONTH FROM trip_start_timestamp) AS trip_start_month,\n",
        "          EXTRACT(HOUR FROM trip_start_timestamp) AS trip_start_hour,\n",
        "          EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS trip_start_day,\n",
        "          UNIX_SECONDS(trip_start_timestamp) AS trip_start_timestamp,\n",
        "          pickup_latitude,\n",
        "          pickup_longitude,\n",
        "          dropoff_latitude,\n",
        "          dropoff_longitude,\n",
        "          trip_miles,\n",
        "          pickup_census_tract,\n",
        "          dropoff_census_tract,\n",
        "          payment_type,\n",
        "          company,\n",
        "          trip_seconds,\n",
        "          dropoff_community_area,\n",
        "          tips\n",
        "        FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
        "        WHERE (ABS(FARM_FINGERPRINT(unique_key)) / 0x7FFFFFFFFFFFFFFF)\n",
        "          < 0.000001\"\"\"\n",
        "\n",
        "# Data location for the CsvExampleGen.\n",
        "CSV_INPUT_PATH = 'gs://ml-pipeline/sample-data/chicago-taxi/data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lc88uEhzAkI"
      },
      "source": [
        "### Create a helper function to construct the TFX pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LL-fA0HqPHn"
      },
      "source": [
        "# Create a helper function to construct a TFX pipeline.\n",
        "def create_tfx_pipeline(\n",
        "    query: Optional[Text] = None,\n",
        "    input_path: Optional[Text] = None,\n",
        "):\n",
        "  \"\"\"Creates an end-to-end Chicago Taxi pipeline in TFX.\"\"\"\n",
        "  if bool(query) == bool(input_path):\n",
        "    raise ValueError('Exact one of query or input_path is expected.')\n",
        "\n",
        "  if query:\n",
        "    example_gen = BigQueryExampleGen(query=query)\n",
        "  else:\n",
        "    example_gen = CsvExampleGen(input_base=input_path)\n",
        "\n",
        "  beam_pipeline_args = [\n",
        "      # Uncomment to use Dataflow.\n",
        "      # '--runner=DataflowRunner',\n",
        "      # '--experiments=shuffle_mode=auto',\n",
        "      '--temp_location=' + os.path.join(PIPELINE_ROOT, 'dataflow', 'temp'),\n",
        "  #     '--region=us-central1',\n",
        "  #     '--disk_size_gb=100',\n",
        "      '--project={}'.format(PROJECT_ID)  # Always needed for BigQueryExampleGen.\n",
        "  ]\n",
        "\n",
        "  module_file = 'gs://ml-pipeline-playground/tfx_taxi_simple/modules/taxi_utils.py'\n",
        "\n",
        "  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
        "  schema_gen = SchemaGen(\n",
        "      statistics=statistics_gen.outputs['statistics'],\n",
        "      infer_feature_shape=False)\n",
        "  example_validator = ExampleValidator(\n",
        "      statistics=statistics_gen.outputs['statistics'],\n",
        "      schema=schema_gen.outputs['schema'])\n",
        "  transform = Transform(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      schema=schema_gen.outputs['schema'],\n",
        "      module_file=module_file)\n",
        "  # Fetch the latest trained model under the same context for warm-starting.\n",
        "  latest_model_resolver = ResolverNode(\n",
        "      instance_name='latest_model_resolver',\n",
        "      resolver_class=latest_artifacts_resolver.LatestArtifactsResolver,\n",
        "      model=channel.Channel(type=standard_artifacts.Model))\n",
        "  trainer = Trainer(\n",
        "      transformed_examples=transform.outputs['transformed_examples'],\n",
        "      schema=schema_gen.outputs['schema'],\n",
        "      base_model=latest_model_resolver.outputs['model'],\n",
        "      transform_graph=transform.outputs['transform_graph'],\n",
        "      train_args=trainer_pb2.TrainArgs(num_steps=10),\n",
        "      eval_args=trainer_pb2.EvalArgs(num_steps=5),\n",
        "      module_file=module_file,\n",
        "  )\n",
        "  # Get the latest blessed model for model validation.\n",
        "  model_resolver = ResolverNode(\n",
        "      instance_name='latest_blessed_model_resolver',\n",
        "      resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
        "      model=channel.Channel(type=standard_artifacts.Model),\n",
        "      model_blessing=channel.Channel(type=standard_artifacts.ModelBlessing))\n",
        "\n",
        "  #   # Set the TFMA config for Model Evaluation and Validation.\n",
        "  #   # This is  \n",
        "  # eval_config = tfma.EvalConfig(\n",
        "  #     model_specs=[tfma.ModelSpec(signature_name='eval')],\n",
        "  #     metrics_specs=[\n",
        "  #         tfma.MetricsSpec(\n",
        "  #             metrics=[tfma.MetricConfig(class_name='ExampleCount')],\n",
        "  #             thresholds={\n",
        "  #                 'binary_accuracy':\n",
        "  #                     tfma.MetricThreshold(\n",
        "  #                         value_threshold=tfma.GenericValueThreshold(\n",
        "  #                             lower_bound={'value': 0.5}),\n",
        "  #                         change_threshold=tfma.GenericChangeThreshold(\n",
        "  #                             direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "  #                             absolute={'value': -1e-10}))\n",
        "  #             })\n",
        "  #     ],\n",
        "  #     slicing_specs=[\n",
        "  #         tfma.SlicingSpec(),\n",
        "  #         tfma.SlicingSpec(feature_keys=['trip_start_hour'])\n",
        "  #     ])\n",
        "\n",
        "  # This eval_config is for blindly blessing the model.\n",
        "  eval_config = tfma.EvalConfig(\n",
        "      model_specs=[\n",
        "          tfma.ModelSpec(signature_name='eval'),\n",
        "      ],\n",
        "      metrics_specs=[\n",
        "          tfma.MetricsSpec(metrics=[\n",
        "              tfma.config.MetricConfig(\n",
        "                  class_name='ExampleCount',\n",
        "                  # Bless the model as long as there are at least one example.\n",
        "                  threshold=tfma.config.MetricThreshold(\n",
        "                      value_threshold=tfma.GenericValueThreshold(\n",
        "                          lower_bound={'value': 0}))),\n",
        "          ]),\n",
        "      ],\n",
        "      slicing_specs=[tfma.SlicingSpec()])\n",
        "\n",
        "  evaluator = Evaluator(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      model=trainer.outputs['model'],\n",
        "      baseline_model=model_resolver.outputs['model'],\n",
        "      eval_config=eval_config)\n",
        "\n",
        "  pusher = Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      model_blessing=evaluator.outputs['blessing'],\n",
        "      push_destination=pusher_pb2.PushDestination(\n",
        "          filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "              base_directory=os.path.join(PIPELINE_ROOT, 'model_serving'))))\n",
        "\n",
        "  components=[\n",
        "      example_gen, statistics_gen, schema_gen, example_validator, transform,\n",
        "      latest_model_resolver, trainer, model_resolver, evaluator, pusher\n",
        "  ]\n",
        "\n",
        "  return tfx_pipeline.Pipeline(\n",
        "      pipeline_name='taxi-pipeline-{}'.format(USER),\n",
        "      pipeline_root=PIPELINE_ROOT,\n",
        "      components=components,\n",
        "      beam_pipeline_args=beam_pipeline_args\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3B_ALPmzOop"
      },
      "source": [
        "### Compile and run the pipeline\n",
        "\n",
        "We'll first use the helper function to create the pipeline, passing it the BigQuery query we defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFxe5RjvqPHq"
      },
      "source": [
        "bigquery_taxi_pipeline = create_tfx_pipeline(query=QUERY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY7viEgkqPHy"
      },
      "source": [
        "Compile the pipeline.  This creates the `pipeline.json` job spec."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iex7qZB9qPHy"
      },
      "source": [
        "runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(\n",
        "    config=kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(\n",
        "        project_id=PROJECT_ID\n",
        "    ),\n",
        "    output_filename='pipeline.json')\n",
        "_ = runner.compile(bigquery_taxi_pipeline, write_out=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGgtF1g-qPH1"
      },
      "source": [
        "Now we're ready to run the pipeline!  We're constructing the client using the API_KEY that you created during setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZmXfxJ5rhCx"
      },
      "source": [
        "from aiplatform.pipelines import client\n",
        "\n",
        "my_client = client.Client(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    api_key=API_KEY\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqKbLBjizlBs"
      },
      "source": [
        "Run the pipeline using the client's `create_run_from_job_spec` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOQXoa5wAEB0"
      },
      "source": [
        "my_client.create_run_from_job_spec('pipeline.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kgtx8-bW7cM"
      },
      "source": [
        "### Monitor the pipeline run in the Cloud Console\n",
        "\n",
        "Once you've deployed the pipeline run, you can monitor it in the [Cloud Console](https://console.cloud.google.com/ai/platform/pipelines) under **AI Platform (Unified)** > **Pipelines**. \n",
        "\n",
        "Click in to the pipeline run to see the run graph, and click on a step to view the job detail and the logs for that step.\n",
        "\n",
        "As you look at the pipeline graph, you'll see that you can inspect the artifacts passed between the pipeline steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjSnifQ1csR5"
      },
      "source": [
        "In the pipeline run's UI, which shows the DAG, you should be able to see that the model is 'blessed' by the evaluator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TezenrizqPH3"
      },
      "source": [
        "Now let's see if the 'blessed' model can\n",
        "be retried by another pipeline job as the baseline model for model validation.\n",
        "\n",
        "First, to expedite the process, let's turn on the cache. Then we'll submit the pipeline for execution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgLUoKtGsYnY"
      },
      "source": [
        "import time\n",
        "\n",
        "bigquery_taxi_pipeline.enable_cache = True\n",
        "runner.compile(bigquery_taxi_pipeline, write_out=True)\n",
        "my_client.create_run_from_job_spec('pipeline.json', name='big-query-taxi-pipeline-{}-2nd-{}'.format(USER, str(int(time.time()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t44-06setIKu"
      },
      "source": [
        "You should be able to see that all the steps before Trainer are cached. And a 'blessed' model is used as the baseline model for the Evaluator component.\n",
        "\n",
        "<a href=\"https://storage.googleapis.com/amy-jo/images/kf-pls/tfx_bq.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kf-pls/tfx_bq.png\" width=\"90%\"/></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEImagWTdJ-g"
      },
      "source": [
        "Next, let's try out a taxi pipeline which uses a file location as the data source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEu-NFSsqPHv"
      },
      "source": [
        "fbeg_taxi_pipeline = create_tfx_pipeline(input_path=CSV_INPUT_PATH)\n",
        "runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(\n",
        "    config=kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(\n",
        "        project_id=PROJECT_ID),\n",
        "    output_filename='pipeline.json')\n",
        "runner.compile(fbeg_taxi_pipeline, write_out=True)\n",
        "my_client.create_run_from_job_spec('pipeline.json', name='file-based-taxi-pipeline-{}-3rd-{}'.format(USER, str(int(time.time()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89fYarRLW7cN"
      },
      "source": [
        "-----------------------------\n",
        "Copyright 2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    }
  ]
}
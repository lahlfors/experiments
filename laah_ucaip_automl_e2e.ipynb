{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Tags","colab":{"name":"laah_ucaip_automl_e2e.ipynb","provenance":[{"file_id":"1yjaJyXU3mIl3vkgih7fK4XCaLyOPGnkG","timestamp":1612816271588},{"file_id":"1zJifCHeFMXZoaVSJV4T3okP920AAO9R-","timestamp":1606851039784}],"collapsed_sections":[],"toc_visible":true},"environment":{"name":"tf2-2-3-gpu.2-3.m56","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}}},"cells":[{"cell_type":"markdown","metadata":{"id":"am1vrCYu1dkq"},"source":["# Unified AI Platform: AutoML Tabular 'end-to-end' workflow\n","\n","## Introduction\n","\n","This example shows how to build an [AutoML Tabular](https://cloud.google.com/ai-platform-unified/docs/training/training) 'end-to-end' workflow for Managed Pipelines, using the [Unified AI Platform](https://cloud.google.com/ai-platform-unified/docs)'s SDK.\n","\n","The pipeline creates a *Dataset* from a [BigQuery](https://cloud.google.com/bigquery/) table, uses it to train a tabular regression model, gets evaluation information about the trained model, and if the model is sufficiently accurate, deploys the model for prediction.\n","\n","<a href=\"https://storage.googleapis.com/amy-jo/images/automl/ucaip_automl_tabular_dag.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/automl/ucaip_automl_tabular_dag.png\" width=\"90%\"/></a>\n","\n","The training can take a few hours, so this example pipeline will take a while to run.\n","\n","### About the dataset and modeling task\n","\n","The  [Cloud Public Datasets Program](https://cloud.google.com/bigquery/public-data/)  makes available public datasets that are useful for experimenting with machine learning. Just as with this “[Explaining model predictions on structured data](https://cloud.google.com/blog/products/ai-machine-learning/explaining-model-predictions-structured-data)” post, we’ll use data that is essentially a join of two public datasets stored in  [BigQuery](https://cloud.google.com/bigquery/) :  [London Bike rentals](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=london_bicycles&page=dataset)  and  [NOAA weather data](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=noaa_gsod&page=dataset) , with some additional processing to clean up outliers and derive additional GIS and day-of-week fields.\n","\n","We’ll use this dataset to build a *regression* model to predict the duration of a bike rental based on information about the start and end stations, the day of the week, the weather on that day, and other data. If we were running a bike rental company, for example, these predictions—and their explanations—could help us anticipate demand and even plan how to stock each location."]},{"cell_type":"markdown","metadata":{"id":"RWACue6PW7bk"},"source":["## Setup\n","\n","Before you run this notebook, ensure that your Google Cloud user account and project are granted access to the Managed Pipelines Experimental. To be granted access to the Managed Pipelines Experimental, fill out this [form](http://go/cloud-mlpipelines-signup) and let your account representative know you have requested access. \n","\n","This notebook is intended to be run on either one of:\n","* [AI Platform Notebooks](https://cloud.google.com/ai-platform-notebooks). See the \"AI Platform Notebooks\" section in the Experimental [User Guide](https://docs.google.com/document/d/1JXtowHwppgyghnj1N1CT73hwD1caKtWkLcm2_0qGBoI/edit?usp=sharing) for more detail on creating a notebook server instance.\n","* [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)\n","\n","If you haven't already enabled the AI Platform API, on the [AI Platform (Unified) Dashboard](https://console.cloud.google.com/ai/platform) page in the Google Cloud Console, click **Enable the AI Platform API**.\n","\n","\n","We'll first install some libraries and set up some variables.\n"]},{"cell_type":"markdown","metadata":{"id":"GAaCPLjgiJrO"},"source":["Set `gcloud` to use your project.  **Edit the following cell before running it**."]},{"cell_type":"code","metadata":{"id":"0GlP_C9mY3Gq"},"source":["PROJECT_ID = 'laah-pipeline'  # <---CHANGE THIS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QkB59nvxyp0-"},"source":["# aju temp testing\n","PROJECT_ID = 'aju-vtests2'  # <---CHANGE THIS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VkWdxe4TXRHk"},"source":["!gcloud config set project {PROJECT_ID}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gckGHdW9iPrq"},"source":["If you're running this notebook on colab, authenticate with your user account:"]},{"cell_type":"code","metadata":{"id":"kZQA0KrfXCvU"},"source":["import sys\n","if 'google.colab' in sys.modules:\n","  from google.colab import auth\n","  auth.authenticate_user()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPw5TVfWYLfQ"},"source":["### Install the KFP SDK and AI Platform Pipelines client library\n","\n","For Managed Pipelines Experimental, you'll need to download a special version of the AI Platform client library."]},{"cell_type":"code","metadata":{"id":"QlPnul5UW7bl"},"source":["!gsutil cp gs://cloud-aiplatform-pipelines/releases/20201123/aiplatform_pipelines_client-0.1.0.caip20201123-py3-none-any.whl ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fX-JOHG_YLfY"},"source":["Then, install the libraries and restart the kernel."]},{"cell_type":"code","metadata":{"id":"4c4l9EWnYLfY"},"source":["if 'google.colab' in sys.modules:\n","  USER_FLAG = ''\n","else:\n","  USER_FLAG = '--user'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-r8250sUlTf8"},"source":["!python3 -m pip install {USER_FLAG} kfp==1.1.2 --upgrade\n","!python3 -m pip install {USER_FLAG} aiplatform_pipelines_client-0.1.0.caip20201123-py3-none-any.whl --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TjvbYuDJms7w"},"source":["!python3 -m pip install {USER_FLAG} google-cloud-aiplatform"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrQxxf97YLfg"},"source":["# Automatically restart kernel after installs \n","# (for this notebook, seems this might be necessary for colab too)\n","import IPython\n","app = IPython.Application.instance()\n","app.kernel.do_shutdown(True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N33S1ikHIOPS"},"source":["The KFP version should be == 1.1.2.\n","\n"]},{"cell_type":"code","metadata":{"id":"a4uvTyimMYOr"},"source":["# Check the KFP version\n","!python3 -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t1GX5KDOUJuI"},"source":["If you're on colab, re-authorize after the kernel restart. **Edit the following cell for your project ID before running it.**"]},{"cell_type":"code","metadata":{"id":"PpkxFp93xBk5"},"source":["import sys\n","if 'google.colab' in sys.modules:\n","  PROJECT_ID = 'laah-pipeline'  # <---CHANGE THIS\n","  !gcloud config set project {PROJECT_ID}\n","  from google.colab import auth\n","  auth.authenticate_user()\n","  USER_FLAG = ''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9p0qgwInrWfW"},"source":["# aju temp testing\n","import sys\n","if 'google.colab' in sys.modules:\n","  PROJECT_ID = 'aju-vtests2'  # <---CHANGE THIS\n","  !gcloud config set project {PROJECT_ID}\n","  from google.colab import auth\n","  auth.authenticate_user()\n","  USER_FLAG = ''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tskC13YxW7b3"},"source":["### Set some variables\n","\n","**Before you run the next cell**, **edit it** to set variables for your project.  See the \"Before you begin\" section of the User Guide for information on creating your API key.  For `BUCKET_NAME`, enter the name of a Cloud Storage (GCS) bucket in your project.  Don't include the `gs://` prefix."]},{"cell_type":"code","metadata":{"id":"iXvbCu2CYvoS"},"source":["PATH=%env PATH\n","%env PATH={PATH}:/home/jupyter/.local/bin\n","\n","# Required Parameters\n","USER = 'laah' # <---CHANGE THIS\n","BUCKET_NAME = 'laah-mp-lab'  # <---CHANGE THIS\n","PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER)\n","\n","PROJECT_ID = 'laah-pipeline'  # <---CHANGE THIS\n","REGION = 'us-central1'\n","API_KEY = 'AIzaSyAkN9WSMWWMuneFZBSuEv7nJrmov2UBVQU'  # <---CHANGE THIS\n","\n","print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hsBFsM5gWlY8"},"source":["## Create the underlying container image used for the pipeline steps\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0s0kBIPtR-_d"},"source":["We'll use Cloud Build to generate the container image used by all the pipeline components in this example. "]},{"cell_type":"code","metadata":{"id":"gy5J9juWW1Vd"},"source":["%%writefile Dockerfile\n","\n","FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\n","\n","RUN pip install -U google-cloud-aiplatform\n","RUN pip install -U google-cloud-storage"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQ3Y9CErXs9M"},"source":["!gcloud builds submit --tag gcr.io/{PROJECT_ID}/custom-container-ucaip:{USER} ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yJhxjByh1dlK"},"source":["## Create the pipeline components\n","\n","In this section we'll define our pipeline components. We'll build Python function-based components.  First we'll define some python functions, then generate component `yaml` files based on those function definitions."]},{"cell_type":"markdown","metadata":{"id":"owsU_9R3ms7w"},"source":["### Create the dataset\n","\n","Create a *Dataset* and ingest data into it from a BigQuery table. This function assumes a BQ table as the source, and would be a bit different if the source was a set of GCS files."]},{"cell_type":"code","metadata":{"id":"b1-GXmdx06Ex"},"source":["from kfp.v2.components import OutputPath\n","\n","def create_dataset_tabular_bigquery_sample(\n","    project: str,\n","    display_name: str,\n","    bigquery_uri: str, # 'bq://aju-dev-demos.london_bikes_weather.bikes_weather',\n","    location: str, # \"us-central1\",\n","    api_endpoint: str, # \"us-central1-aiplatform.googleapis.com\",\n","    timeout: int, # 500,\n","    dataset_id: OutputPath('String'),\n","):\n","\n","  import logging\n","  import subprocess\n","  import time\n","\n","  from google.cloud import aiplatform\n","  from google.protobuf import json_format\n","  from google.protobuf.struct_pb2 import Value\n","\n","  logging.getLogger().setLevel(logging.INFO)\n","\n","  client_options = {\"api_endpoint\": api_endpoint}\n","  # Initialize client that will be used to create and send requests.\n","  client = aiplatform.gapic.DatasetServiceClient(client_options=client_options)\n","  metadata_dict = {\"input_config\": {\"bigquery_source\": {\"uri\": bigquery_uri}}}\n","  metadata = json_format.ParseDict(metadata_dict, Value())\n","\n","  dataset = {\n","      \"display_name\": display_name,\n","      \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/tabular_1.0.0.yaml\",\n","      \"metadata\": metadata,\n","  }\n","  parent = f\"projects/{project}/locations/{location}\"\n","  response = client.create_dataset(parent=parent, dataset=dataset)\n","  print(\"Long running operation:\", response.operation.name)\n","  create_dataset_response = response.result(timeout=timeout)\n","  logging.info(\"create_dataset_response: %s\", create_dataset_response)\n","  path_components = create_dataset_response.name.split('/')\n","  logging.info('got dataset id: %s', path_components[-1])\n","  # write the dataset id as output\n","  with open('temp.txt', \"w\") as outfile:\n","    outfile.write(path_components[-1])\n","  subprocess.run(['gsutil', 'cp', 'temp.txt', dataset_id])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pWWpp0J4vnDO"},"source":["### Train an AutoML tabular regression model\n","\n","We'll train an AutoML tabular model on the dataset.  To set up the training job, we need to provide the target (label) column and a dict of the input transformations: which fields to use as model inputs, and the input types.  These will be set later on in the notebook, when we run the pipeline.   \n","The target column will be set to `duration`, a numeric field. This function definition assumes a *regression* model, and would be a bit different if we were training a classification model instead."]},{"cell_type":"code","metadata":{"id":"A0XvsYNM-Dey"},"source":["from kfp.v2.components import OutputPath\n","\n","def training_tabular_regression(\n","    project: str,\n","    display_name: str,\n","    dataset_id: str,\n","    model_prefix: str,\n","    target_column: str,\n","    transformations_str: str,\n","    location: str, # \"us-central1\",\n","    api_endpoint: str, # \"us-central1-aiplatform.googleapis.com\"\n","    model_id: OutputPath('String'),\n","    model_dispname: OutputPath('String')\n","):\n","  import json\n","  import logging\n","  import subprocess\n","  import time\n","  from google.cloud import aiplatform\n","  from google.protobuf import json_format\n","  from google.protobuf.struct_pb2 import Value\n","  from google.cloud.aiplatform_v1beta1.types import pipeline_state\n","\n","  SLEEP_INTERVAL = 100\n","\n","  logging.getLogger().setLevel(logging.INFO)\n","  logging.info('using dataset id: %s', dataset_id)\n","  client_options = {\"api_endpoint\": api_endpoint}\n","  # Initialize client that will be used to create and send requests.\n","  client = aiplatform.gapic.PipelineServiceClient(client_options=client_options)\n","  # set the columns used for training and their data types\n","  transformations = json.loads(transformations_str)\n","  logging.info('using transformations: %s', transformations)\n","\n","  training_task_inputs_dict = {\n","        # required inputs\n","        \"targetColumn\": target_column,\n","        \"predictionType\": \"regression\",\n","        \"transformations\": transformations,\n","        \"trainBudgetMilliNodeHours\": 2000,\n","        \"disableEarlyStopping\": False,\n","        \"optimizationObjective\": \"minimize-rmse\",\n","  }\n","  training_task_inputs = json_format.ParseDict(training_task_inputs_dict, Value())\n","  model_display_name = '{}_{}'.format(model_prefix, str(int(time.time())))\n","\n","  training_pipeline = {\n","        \"display_name\": display_name,\n","        \"training_task_definition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_tabular_1.0.0.yaml\",\n","        \"training_task_inputs\": training_task_inputs,\n","        \"input_data_config\": {\n","            \"dataset_id\": dataset_id,\n","            \"fraction_split\": {\n","                \"training_fraction\": 0.8,\n","                \"validation_fraction\": 0.1,\n","                \"test_fraction\": 0.1,\n","            },\n","        },\n","        \"model_to_upload\": {\"display_name\": model_display_name},\n","  }\n","  parent = f\"projects/{project}/locations/{location}\"\n","  response = client.create_training_pipeline(\n","        parent=parent, training_pipeline=training_pipeline\n","  )\n","  training_pipeline_name = response.name\n","  logging.info(\"pipeline name: %s\", training_pipeline_name)\n","  # Poll periodically until training completes\n","  while True:\n","    mresponse = client.get_training_pipeline(name=training_pipeline_name)\n","    logging.info('mresponse: %s', mresponse)\n","    logging.info('job state: %s', mresponse.state)\n","    if mresponse.state == pipeline_state.PipelineState.PIPELINE_STATE_SUCCEEDED:\n","      logging.info('training finished')\n","      # write some outputs once finished\n","      model_name = mresponse.model_to_upload.name \n","      logging.info('got model name: %s', model_name)\n","      with open('temp.txt', \"w\") as outfile:\n","        outfile.write(model_name)\n","      subprocess.run(['gsutil', 'cp', 'temp.txt', model_id])\n","      with open('temp2.txt', \"w\") as outfile:\n","        outfile.write(model_display_name)\n","      subprocess.run(['gsutil', 'cp', 'temp2.txt', model_dispname])      \n","      break\n","    else:\n","      time.sleep(SLEEP_INTERVAL)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"84bKcN7Z-En-"},"source":["### Get evaluation information for the trained model\n","\n","..."]},{"cell_type":"code","metadata":{"id":"0NQrhIsrv3le"},"source":["\n","def get_model_evaluation_tabular(\n","    project: str,\n","    model_id: str,\n","    location: str, #\"us-central1\",\n","    api_endpoint: str, # \"us-central1-aiplatform.googleapis.com\",\n","    eval_info: OutputPath('String'),\n","):\n","  import json\n","  import logging\n","  import subprocess  \n","  from google.cloud import aiplatform\n","\n","  def get_eval_id(client, model_name):\n","    from google.protobuf.json_format import MessageToDict\n","    response = client.list_model_evaluations(parent=model_name)\n","    for evaluation in response:\n","        print(\"model_evaluation\")\n","        print(\" name:\", evaluation.name)\n","        print(\" metrics_schema_uri:\", evaluation.metrics_schema_uri)\n","        metrics = MessageToDict(evaluation._pb.metrics)\n","        for metric in metrics.keys():\n","            logging.info('metric: %s, value: %s', metric, metrics[metric])\n","        metrics_str = json.dumps(metrics)\n","\n","    return (evaluation.name, metrics_str)  # for regression, only one slice\n","\n","  logging.getLogger().setLevel(logging.INFO)\n","\n","  client_options = {\"api_endpoint\": api_endpoint}\n","  # Initialize client that will be used to create and send requests.\n","  client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n","  eval_name, metrics_str = get_eval_id(client, model_id)\n","  logging.info('got evaluation name: %s', eval_name)\n","  logging.info('got metrics dict string: %s', metrics_str)\n","  with open('temp.txt', \"w\") as outfile:\n","    outfile.write(metrics_str)\n","  subprocess.run(['gsutil', 'cp', 'temp.txt', eval_info])  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T6BXo0Uj9uYX"},"source":["### Create a serving endpoint\n","\n","Here we're creating a serving endpoint to which we'll deploy the trained model.  If an existing endpoint path is given as an arg, we'll use that instead of creating a new endpoint. "]},{"cell_type":"code","metadata":{"id":"33c4zW-v9wi8"},"source":["from kfp.v2.components import OutputPath\n","\n","def create_endpoint(\n","    project: str,\n","    display_name: str,\n","    endpoint_path: str,    \n","    location: str, # \"us-central1\",\n","    api_endpoint: str, # \"us-central1-aiplatform.googleapis.com\",\n","    timeout: int,\n","    endpoint_id: OutputPath('String'),\n","\n","):\n","  import logging\n","  import subprocess  \n","  from google.cloud import aiplatform\n","\n","  logging.getLogger().setLevel(logging.INFO)\n","  if endpoint_path == 'new':  # then create new endpoint, using given display name\n","    logging.info('creating new endpoint with display name: %s', display_name)\n","    client_options = {\"api_endpoint\": api_endpoint}\n","    # Initialize client that will be used to create and send requests.\n","    client = aiplatform.gapic.EndpointServiceClient(client_options=client_options)\n","    endpoint = {\"display_name\": display_name}\n","    parent = f\"projects/{project}/locations/{location}\"\n","    response = client.create_endpoint(parent=parent, endpoint=endpoint)\n","    logging.info(\"Long running operation: %s\", response.operation.name)\n","    create_endpoint_response = response.result(timeout=timeout)\n","    logging.info(\"create_endpoint_response: %s\", create_endpoint_response)\n","    endpoint_name = create_endpoint_response.name \n","    logging.info('endpoint name: %s', endpoint_name)\n","  else:  # otherwise, use given endpoint path expression (TODO: add error checking)\n","    logging.info('using existing endpoint: %s', endpoint_path)\n","    endpoint_name = endpoint_path\n","  # write the endpoint name (path expression) as output\n","  with open('temp.txt', \"w\") as outfile:\n","    outfile.write(endpoint_name)\n","  subprocess.run(['gsutil', 'cp', 'temp.txt', endpoint_id])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kDgGLoLQv_s3"},"source":["### Deploy the trained model for serving\n","\n","This function deploys the trained model for serving.  It supports a simple 'gating' on model quality: \n","We're passing in the model eval metrics info and will compare that with a dict of metrics threshold values.  If the model doesn't meet the given threshold value(s), it won't be deployed. "]},{"cell_type":"code","metadata":{"id":"loeYWroDwC3B"},"source":["def deploy_automl_tabular_model(\n","    project: str,\n","    endpoint_name: str,\n","    model_name: str,\n","    deployed_model_display_name: str,\n","    eval_info: str,\n","    location: str = \"us-central1\",\n","    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n","    timeout: int = 7200,\n","    thresholds_dict_str: str = '{\"meanAbsoluteError\": 470}'\n","):\n","  import json\n","  import logging\n","  from google.cloud import aiplatform\n","\n","  # check the model metrics against the given thresholds dict\n","  def regression_thresholds_check(metrics_dict, thresholds_dict):\n","    for k, v in thresholds_dict.items():\n","      logging.info('k {}, v {}'.format(k, v))\n","      if k in ['rootMeanSquaredError', 'meanAbsoluteError']:  # lower is better\n","        if metrics_dict[k] > v:  # if over threshold\n","          logging.info('{} > {}; returning False'.format(\n","              metrics_dict[k], v))\n","          return False\n","      elif k in ['rSquared']:  # higher is better\n","        if metrics_dict[k] < v:  # if under threshold\n","          logging.info('{} < {}; returning False'.format(\n","              metrics_dict[k], v))\n","          return False\n","      else:  # unhandled key in thresholds dict\n","        # TODO: should the default instead be to deploy?\n","        logging.info('unhandled threshold key %s; not deploying', k)\n","        return False\n","    logging.info('threshold checks passed.')\n","    return True  \n","\n","  logging.getLogger().setLevel(logging.INFO)\n","  metrics_dict = json.loads(eval_info)\n","  thresholds_dict = json.loads(thresholds_dict_str)\n","  logging.info('got metrics dict: %s', metrics_dict)\n","  logging.info('got thresholds dict: %s', thresholds_dict)\n","  deploy = regression_thresholds_check(metrics_dict, thresholds_dict)\n","  if not deploy:\n","    # then don't deploy the model\n","    logging.warning('model is not accurate enough to deploy')\n","    return \n","\n","  client_options = {\"api_endpoint\": api_endpoint}\n","  # Initialize client that will be used to create and send requests.\n","  client = aiplatform.gapic.EndpointServiceClient(client_options=client_options)\n","  deployed_model = {\n","      # format: 'projects/{project}/locations/{location}/models/{model}'\n","      \"model\": model_name,\n","      \"display_name\": deployed_model_display_name,\n","      \"dedicated_resources\": {\n","          \"min_replica_count\": 1,\n","          \"machine_spec\": {\n","              \"machine_type\": \"n1-standard-8\",\n","              # Accelerators can be used only if the model specifies a GPU image.\n","              # 'accelerator_type': aiplatform.AcceleratorType.NVIDIA_TESLA_K80,\n","              # 'accelerator_count': 1,\n","          },\n","      }        \n","  }\n","  # key '0' assigns traffic for the newly deployed model\n","  # Traffic percentage values must add up to 100\n","  # Leave dictionary empty if endpoint should not accept any traffic\n","  traffic_split = {\"0\": 100}\n","  response = client.deploy_model(\n","      endpoint=endpoint_name, deployed_model=deployed_model, traffic_split=traffic_split\n","  )\n","  print(\"Long running operation:\", response.operation.name)\n","  deploy_model_response = response.result(timeout=timeout)\n","  print(\"deploy_model_response:\", deploy_model_response)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"naBzue0Tv5Sr"},"source":["### Create components from the python functions\n","\n","Now we'll use the `func_to_container_op` method to create `yaml` component files for the functions above.  In all cases we'll use as the `base_image` the container we built above.  \n","\n","The `yaml` component files make it easy to version-track and share component definitions. "]},{"cell_type":"code","metadata":{"id":"j31DEZ1q4sXS"},"source":["from kfp.v2 import components\n","from kfp.v2 import dsl\n","from kfp.v2 import compiler\n","\n","components.func_to_container_op(create_dataset_tabular_bigquery_sample,\n","      output_component_file='tables_create_dataset_component.yaml', \n","      base_image='gcr.io/{}/custom-container-ucaip:{}'.format(PROJECT_ID, USER))\n","\n","components.func_to_container_op(training_tabular_regression,\n","      output_component_file='tables_train_component.yaml', \n","      base_image='gcr.io/{}/custom-container-ucaip:{}'.format(PROJECT_ID, USER))\n","\n","components.func_to_container_op(get_model_evaluation_tabular,\n","      output_component_file='tables_eval_component.yaml', \n","      base_image='gcr.io/{}/custom-container-ucaip:{}'.format(PROJECT_ID, USER))\n","\n","components.func_to_container_op(create_endpoint,\n","      output_component_file='tables_endpoint_component.yaml', \n","      base_image='gcr.io/{}/custom-container-ucaip:{}'.format(PROJECT_ID, USER))\n","\n","components.func_to_container_op(deploy_automl_tabular_model,\n","      output_component_file='tables_deploy_component.yaml', \n","      base_image='gcr.io/{}/custom-container-ucaip:{}'.format(PROJECT_ID, USER))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_8XbvBUVqjV4"},"source":["## Define and run a pipeline using the components\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-6WjcWl8qo8r"},"source":["First, we'll create pipeline ops from the components, using the `load_component_from_file` method.\n","\n","While we're not using it here, there is also a `load_component_from_url` method, which is handy if your component files are checked into a repo or otherwise stored online. (For GitHub files, use the 'raw' URL)."]},{"cell_type":"code","metadata":{"id":"B3tnkmMn4kkE"},"source":["import json\n","from kfp.v2 import dsl\n","from kfp.v2 import compiler\n","from kfp.v2 import components\n","\n","create_dataset_op = components.load_component_from_file(\n","  './tables_create_dataset_component.yaml'\n","  )\n","\n","train_op = components.load_component_from_file(\n","  './tables_train_component.yaml'\n","  )\n","\n","eval_op = components.load_component_from_file(\n","  './tables_eval_component.yaml'\n","  )\n","\n","create_endpoint_op = components.load_component_from_file(\n","  './tables_endpoint_component.yaml'\n","  )\n","\n","deploy_op = components.load_component_from_file(\n","  './tables_deploy_component.yaml'\n","  )\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1Hz_nyiqvqW"},"source":["Next, we'll define the pipeline, using the ops defined above."]},{"cell_type":"code","metadata":{"id":"yQHAp_S1EVeS"},"source":["import json\n","# We'll use this transformation specification as an arg for the training step.\n","TRANSFORMATIONS = [\n","    {\"auto\": {\"column_name\": \"bike_id\"}},\n","    {\"auto\": {\"column_name\": \"day_of_week\"}},\n","    {\"auto\": {\"column_name\": \"dewp\"}},\n","    {\"auto\": {\"column_name\": \"duration\"}},\n","    {\"auto\": {\"column_name\": \"end_latitude\"}},\n","    {\"auto\": {\"column_name\": \"end_longitude\"}},\n","    {\"categorical\": {\"column_name\": \"end_station_id\"}},\n","    {\"auto\": {\"column_name\": \"euclidean\"}},\n","    {\"categorical\": {\"column_name\": \"loc_cross\"}},\n","    {\"auto\": {\"column_name\": \"max\"}},\n","    {\"auto\": {\"column_name\": \"min\"}},\n","    {\"auto\": {\"column_name\": \"prcp\"}},\n","    {\"auto\": {\"column_name\": \"start_latitude\"}},\n","    {\"auto\": {\"column_name\": \"start_longitude\"}},\n","    {\"categorical\": {\"column_name\": \"start_station_id\"}},\n","    {\"auto\": {\"column_name\": \"temp\"}},\n","    {\"timestamp\": {\"column_name\": \"ts\"}}\n","]\n","TRANSFORMATIONS_STR = json.dumps(TRANSFORMATIONS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YJiEXWC38l_z"},"source":["Some of the pipeline steps take as inputs the outputs from other ops.\n","\n","Note that the `create_endpoint` step has no ordering constraints and can run right away, so it can run concurrently with the steps that create a dataset and train a model.  "]},{"cell_type":"code","metadata":{"id":"D9nzvrTL6aR3"},"source":["@dsl.pipeline(\n","  name='ucaip-automl-tables',\n","  description='Demonstrate a ucaip AutoML Tables workflow'\n",")\n","def automl_tables( \n","  gcp_project_id: str = 'laah-pieline',\n","  gcp_region: str = 'us-central1',\n","  dataset_display_name: str = 'mptest1612815613',\n","  api_endpoint: str = 'us-central1-aiplatform.googleapis.com',\n","  timeout: int = 2000,\n","  bigquery_uri: str = 'bq://aju-dev-demos.london_bikes_weather.bikes_weather',\n","  target_col_name: str = 'duration',\n","  time_col_name: str = 'none',    \n","  transformations: str = TRANSFORMATIONS_STR,\n","  train_budget_milli_node_hours: int = 1000,\n","  model_prefix: str = 'bwmodel',    \n","  # optimization_objective: str = 'minimize-rmse', \n","  training_display_name: str = 'laah-training',\n","  endpoint_display_name = 'laah-endpoint',\n","  # if set to other than 'new', use the given endpoint path rather than create new endpoint.  \n","  endpoint_path:str = 'new',\n","  thresholds_dict_str = '{\"meanAbsoluteError\": 470}'\n","  ):\n","\n","  create_dataset = create_dataset_op(\n","    gcp_project_id,\n","    dataset_display_name,\n","    bigquery_uri,\n","    gcp_region,\n","    api_endpoint,\n","    timeout\n","    )\n","  \n","  train = train_op(\n","    gcp_project_id,\n","    training_display_name,\n","    create_dataset.outputs['dataset_id'],\n","    model_prefix,\n","    target_col_name,\n","    transformations,\n","    gcp_region,\n","    api_endpoint\n","    )\n","  \n","  eval = eval_op(\n","    gcp_project_id,\n","    train.outputs['model_id'],\n","    gcp_region,\n","    api_endpoint\n","    )\n","  \n","  create_endpoint = create_endpoint_op(\n","    gcp_project_id,\n","    dataset_display_name,\n","    endpoint_path,\n","    gcp_region,\n","    api_endpoint,\n","    timeout\n","  )\n","\n","  deploy = deploy_op(\n","    gcp_project_id,\n","    create_endpoint.outputs['endpoint_id'],\n","    train.outputs['model_id'],\n","    train.outputs['model_dispname'],\n","    eval.outputs['eval_info'],\n","    gcp_region,\n","    api_endpoint,\n","    timeout,\n","    thresholds_dict_str\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BeS7LA_1ZIMa"},"source":["Compile the pipeline..."]},{"cell_type":"code","metadata":{"id":"ot0UW7dT7WAZ"},"source":["compiler.Compiler().compile(pipeline_func=automl_tables,\n","                            pipeline_root=PIPELINE_ROOT,\n","                            output_path='automl_pipeline_spec.json')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RDgxW1pBW1H"},"source":["... then run it."]},{"cell_type":"code","metadata":{"id":"SDUsSmiz7WAZ"},"source":["import time\n","from aiplatform.pipelines import client\n","\n","api_client = client.Client(project_id=PROJECT_ID, region=REGION, api_key=API_KEY)\n","display_name = 'mptest{}'.format(str(int(time.time())))\n","print(display_name)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKfoh_3jB0I3"},"source":["Note that we can define pipeline input values via the `parameter_values` arg."]},{"cell_type":"code","metadata":{"id":"J8SO7gEi7WAZ"},"source":["result = api_client.create_run_from_job_spec(\n","          job_spec_path='automl_pipeline_spec.json',\n","#           pipeline_root=PIPELINE_ROOT,  # you can add this arg if you want to override the compiled value\n","          parameter_values={'gcp_project_id': '{}'.format(PROJECT_ID),\n","                           'dataset_display_name': display_name,\n","                            'endpoint_display_name': display_name,\n","                            'training_display_name': display_name,\n","                            'thresholds_dict_str': '{\"meanAbsoluteError\": 470}'\n","                           })"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jLV5VGAk3ZHi"},"source":["Visit the running pipeline job in the Cloud Console by clicking the link above. As it runs, you should see a graph like the following.  \n","\n","<a href=\"https://storage.googleapis.com/amy-jo/images/automl/ucaip_automl_tabular_pipeline_in_progress.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/automl/ucaip_automl_tabular_pipeline_in_progress.png\" width=\"90%\"/></a>\n","\n","You can view and manage information about your dataset, model, and endpoint in the [Cloud Console](https://console.cloud.google.com/ai/platform/models) as well.\n"]},{"cell_type":"markdown","metadata":{"id":"5kUmXtBAjZnk"},"source":["## (TODO) Using your deployed model for prediction\n","\n","..."]},{"cell_type":"code","metadata":{"id":"vtY9fu5snqpr"},"source":["from google.cloud import aiplatform\n","\n","def predict_custom_model_sample(endpoint: str, instance: dict, parameters_dict: dict):\n","    client_options = dict(api_endpoint=\"us-central1-prediction-aiplatform.googleapis.com\")\n","    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n","\n","    from google.protobuf import json_format\n","    from google.protobuf.struct_pb2 import Value\n","\n","    # The format of the parameters must be consistent with what the model expects.\n","    parameters = json_format.ParseDict(parameters_dict, Value())\n","\n","    # The format of the instances must be consistent with what the model expects.\n","    instances_list = [instance]\n","    instances = [json_format.ParseDict(s, Value()) for s in instances_list]\n","    response = client.predict(\n","        endpoint=endpoint, instances=instances, parameters=parameters\n","    )\n","\n","    print(\"response\")\n","    print(\" deployed_model_id:\", response.deployed_model_id)\n","    predictions = response.predictions\n","    print(\"predictions\")\n","    for prediction in predictions:\n","        print(\" prediction:\", dict(prediction))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxh_IkolnvPX"},"source":["endpoint_path = \"projects/467744782358/locations/us-central1/endpoints/6770352799193497600\"  # aju temp testing\n","instance1 =  {\n","      \"bike_id\": \"5373\",\n","      \"day_of_week\": \"3\",\n","      \"end_latitude\": 51.52059681,\n","      \"end_longitude\": -0.116688468,\n","      \"end_station_id\": \"68\",\n","      \"euclidean\": 3589.5146210024977,\n","      \"loc_cross\": \"POINT(-0.07 51.52)POINT(-0.12 51.52)\",\n","      \"max\": 44.6,\n","      \"min\": 34.0,\n","      \"prcp\": 0,\n","      \"ts\": \"1480407420\",\n","      \"start_latitude\": 51.52388,\n","      \"start_longitude\": -0.065076,\n","      \"start_station_id\": \"445\",\n","      \"temp\": 38.2,\n","      \"dewp\": 28.6\n","    }\n","\n","predict_custom_model_sample(\n","    endpoint_path,\n","    instance1, {}\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"89fYarRLW7cN"},"source":["-----------------------------\n","Copyright 2020 Google LLC\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","     http://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License."]}]}